{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f575f58-292c-43e6-a0fd-0171ee68c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "140a3c5e-3b40-4aa6-be8e-051741155407",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = r\"C:\\Users\\Elias\\Final Project\\EJ_screen_data_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2edff902-5a3f-46d3-bcb5-ee1f6ca74dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we want to keep from each file\n",
    "cols_needed = [\n",
    "    \"ID\",\n",
    "    \"P_PTRAF\",   # Traffic proximity percentile\n",
    "    \"DSLPM\",     # Diesel PM\n",
    "    \"CANCER\",    # Cancer risk\n",
    "    \"RESP\",      # Respiratory hazard\n",
    "    \"VULEOPCT\"   # EJ index\n",
    "]\n",
    "\n",
    "# Renaming\n",
    "rename_map = {\n",
    "    \"P_PTRAF\": \"traffic_pct\",\n",
    "    \"DSLPM\": \"diesel_pm\",\n",
    "    \"CANCER\": \"cancer_risk\",\n",
    "    \"RESP\": \"resp_hazard\",\n",
    "    \"VULEOPCT\": \"ej_index\"\n",
    "}\n",
    "\n",
    "numeric_cols = [\"traffic_pct\", \"diesel_pm\", \"cancer_risk\", \"resp_hazard\", \"ej_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72fe018-3c8b-4294-a180-ed6fc3959b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_files():\n",
    "    \"\"\"Load all EJScreen CSV files and combine them\"\"\"\n",
    "    \n",
    "    print(\"Step 1: Loading EJScreen Files\")\n",
    "   \n",
    "    csv_files = glob.glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "    print(f\"\\nFound {len(csv_files)} files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {csv_folder}\")\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        print(f\"\\nProcessing: {os.path.basename(file)}\")\n",
    "        \n",
    "        # Read file as string dtype to avoid mixed type warnings\n",
    "        df = pd.read_csv(file, dtype=str, low_memory=False)\n",
    "        \n",
    "        # Check what columns exist\n",
    "        print(f\"  Available columns: {list(df.columns[:10])}...\")  # Show first 10 columns\n",
    "        \n",
    "        # Check if ID column exists\n",
    "        if \"ID\" not in df.columns:\n",
    "            print(f\" 'ID' column not found, skipping this file\")\n",
    "            continue\n",
    "        \n",
    "        # Extract year from filename (e.g., \"2018\" from \"EJSCREEN_2018_...\")\n",
    "        year = \"\".join([c for c in os.path.basename(file) if c.isdigit()])[:4]\n",
    "        \n",
    "        # Filter for LA County census tracts (FIPS code starts with 06037)\n",
    "        df = df[df[\"ID\"].str.startswith(\"06037\", na=False)]\n",
    "        \n",
    "        # Keep only needed columns that exist\n",
    "        existing_cols = [c for c in cols_needed if c in df.columns]\n",
    "        print(f\"  Columns found: {existing_cols}\")\n",
    "        \n",
    "        if len(existing_cols) < 2:  # At least ID + 1 data column\n",
    "            print(f\" Not enough required columns found, skipping this file\")\n",
    "            continue\n",
    "        \n",
    "        df = df[existing_cols]\n",
    "        \n",
    "        # Rename columns\n",
    "        df = df.rename(columns= rename_map)\n",
    "        \n",
    "        # Add year column\n",
    "        df[\"YEAR\"] = year\n",
    "        \n",
    "        print(f\"  Tracts found: {len(df)}\")\n",
    "        df_list.append(df)\n",
    "    \n",
    "    if not df_list:\n",
    "        raise ValueError(\"No valid data files were processed\")\n",
    "    \n",
    "    # Combine all years\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\n Combined dataset: {combined_df.shape[0]} rows, {combined_df.shape[1]} columns\")\n",
    "    print(f\"\\nYears included:\")\n",
    "    print(combined_df[\"YEAR\"].value_counts().sort_index())\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def convert_to_numeric(df):\n",
    "    \"\"\"Converting numeric columns from string to float\"\"\"\n",
    "    print(\"Step 2: Converting to numeric\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            before_na = df[col].isna().sum()\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            after_na = df[col].isna().sum()\n",
    "            \n",
    "            if after_na > before_na:\n",
    "                print(f\"\\n{col}:\")\n",
    "                print(f\"  New NaN values from conversion: {after_na - before_na}\")\n",
    "    \n",
    "    print(\"\\n All numeric columns converted\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2830086c-9d78-4ee3-9332-955b3c645dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tract_averages(df):\n",
    "    \"\"\"Calculating 2018-2022 average for each census tract\"\"\"\n",
    "    print(\"Step 3: Calculating Tract Averages (2018-2022)\")\n",
    "    \n",
    "    avg_df = df.groupby(\"ID\")[numeric_cols].mean().reset_index()\n",
    "    \n",
    "    print(f\" Unique tracts: {len(avg_df)}\")\n",
    "    \n",
    "    # Renaming to indicate these are averages\n",
    "    avg_df = avg_df.rename(columns={\n",
    "        \"traffic_pct\": \"avg_traffic_pct\",\n",
    "        \"diesel_pm\": \"avg_diesel_pm\",\n",
    "        \"cancer_risk\": \"avg_cancer_risk\",\n",
    "        \"resp_hazard\": \"avg_resp_hazard\",\n",
    "        \"ej_index\": \"avg_ej_index\"\n",
    "    })\n",
    "    \n",
    "    return avg_df\n",
    "\n",
    "def format_tract_ids(df):\n",
    "    \"\"\"Ensuring tract IDs are properly formatted as 12-digit strings\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Step 4: Formating Tract IDs\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nBefore formatting - ID examples:\")\n",
    "    print(df[\"ID\"].head(3).tolist())\n",
    "    \n",
    "    # Convert to string and pad with zeros to 12 digits\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str).str.zfill(12)\n",
    "    \n",
    "    print(f\"\\nAfter formatting - ID examples:\")\n",
    "    print(df[\"ID\"].head(3).tolist())\n",
    "    \n",
    "    # Sort by tract ID\n",
    "    df = df.sort_values(\"ID\").reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n All tract IDs formatted and sorted\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f7c8bf5-1ade-4569-b75f-aad9641110a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Handling missing values\"\"\"\n",
    "    print(\"Step 5: Handling Missing Values\")\n",
    "    avg_cols = [\"avg_traffic_pct\", \"avg_diesel_pm\", \"avg_cancer_risk\", \n",
    "                \"avg_resp_hazard\", \"avg_ej_index\"]\n",
    "    \n",
    "    print(\"\\nMissing values before imputation:\")\n",
    "    for col in avg_cols:\n",
    "        n_missing = df[col].isna().sum()\n",
    "        pct_missing = (n_missing / len(df)) * 100\n",
    "        print(f\"  {col}: {n_missing} ({pct_missing:.2f}%)\")\n",
    "    \n",
    "    # Spatial nearest-neighbor imputation for missing traffic\n",
    "    print(\"\\nApplying spatial nearest-neighbor imputation for avg_traffic_pct...\")\n",
    "    df[\"avg_traffic_pct\"] = fill_with_neighbors(df[\"avg_traffic_pct\"])\n",
    "    \n",
    "    # County-wide median for other variables\n",
    "    print(\"\\nApplying county-wide median imputation for other variables...\")\n",
    "    median_cols = [\"avg_diesel_pm\", \"avg_cancer_risk\", \"avg_resp_hazard\"]\n",
    "    \n",
    "    for col in median_cols:\n",
    "        median_val = df[col].median()\n",
    "        n_filled = df[col].isna().sum()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\"  {col}: filled {n_filled} values with median = {median_val:.2f}\")\n",
    "    \n",
    "    print(\"\\nMissing values after imputation:\")\n",
    "    for col in avg_cols:\n",
    "        n_missing = df[col].isna().sum()\n",
    "        print(f\"  {col}: {n_missing}\")\n",
    "    \n",
    "    if df[avg_cols].isna().any().any():\n",
    "        print(\"\\nMissing values remain\")\n",
    "    else:\n",
    "        print(\"\\n No missing values\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fill_with_neighbors(series):\n",
    "    \"\"\"Fill missing values using spatial nearest neighbors (adjacent tracts)\"\"\"\n",
    "    series = series.copy()\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        if pd.isna(series.iloc[i]):\n",
    "            neighbors = []\n",
    "            \n",
    "            # Check previous tract\n",
    "            if i > 0 and not pd.isna(series.iloc[i-1]):\n",
    "                neighbors.append(series.iloc[i-1])\n",
    "            \n",
    "            # Check next tract\n",
    "            if i < len(series)-1 and not pd.isna(series.iloc[i+1]):\n",
    "                neighbors.append(series.iloc[i+1])\n",
    "            \n",
    "            # Fill with average of available neighbors\n",
    "            if neighbors:\n",
    "                series.iloc[i] = np.mean(neighbors)\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0605a2ba-4c8a-4660-9b18-ed3227a307c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading EJScreen Files\n",
      "\n",
      "Found 5 files:\n",
      "  - EJSCREEN_2019_USPR.csv\n",
      "  - EJSCREEN_2020_USPR.csv\n",
      "  - EJSCREEN_2021_USPR.csv\n",
      "  - EJSCREEN_2022_Supplemental_with_AS_CNMI_GU_VI.csv\n",
      "  - EJSCREEN_Full_USPR_2018.csv\n",
      "\n",
      "Processing: EJSCREEN_2019_USPR.csv\n",
      "  Available columns: ['OBJECTID', 'ID', 'ACSTOTPOP', 'ACSIPOVBAS', 'ACSEDUCBAS', 'ACSTOTHH', 'ACSTOTHU', 'MINORPOP', 'MINORPCT', 'LOWINCOME']...\n",
      "  Columns found: ['ID', 'P_PTRAF', 'DSLPM', 'CANCER', 'RESP', 'VULEOPCT']\n",
      "  Tracts found: 6425\n",
      "\n",
      "Processing: EJSCREEN_2020_USPR.csv\n",
      "  Available columns: ['OBJECTID', 'ID', 'ACSTOTPOP', 'ACSIPOVBAS', 'ACSEDUCBAS', 'ACSTOTHH', 'ACSTOTHU', 'MINORPOP', 'MINORPCT', 'LOWINCOME']...\n",
      "  Columns found: ['ID', 'P_PTRAF', 'DSLPM', 'CANCER', 'RESP', 'VULEOPCT']\n",
      "  Tracts found: 6425\n",
      "\n",
      "Processing: EJSCREEN_2021_USPR.csv\n",
      "  Available columns: ['OBJECTID', 'ID', 'ACSTOTPOP', 'ACSIPOVBAS', 'ACSEDUCBAS', 'ACSTOTHH', 'ACSTOTHU', 'ACSUNEMPBAS', 'MINORPOP', 'MINORPCT']...\n",
      "  Columns found: ['ID', 'P_PTRAF', 'DSLPM', 'CANCER', 'RESP', 'VULEOPCT']\n",
      "  Tracts found: 6425\n",
      "\n",
      "Processing: EJSCREEN_2022_Supplemental_with_AS_CNMI_GU_VI.csv\n",
      "  Available columns: ['38356200', '40.292867320261401', '0.036115020405134', '30.000000000000000', '0.400000000000000', '270.171898870905977', '36', '0.057416267942584', '0.069529787681032', '0.102823903631106']...\n",
      " 'ID' column not found, skipping this file\n",
      "\n",
      "Processing: EJSCREEN_Full_USPR_2018.csv\n",
      "  Available columns: ['OBJECTID', 'ID', 'ACSTOTPOP', 'ACSIPOVBAS', 'ACSEDUCBAS', 'ACSTOTHH', 'ACSTOTHU', 'MINORPOP', 'MINORPCT', 'LOWINCOME']...\n",
      "  Columns found: ['ID', 'P_PTRAF', 'DSLPM', 'CANCER', 'RESP', 'VULEOPCT']\n",
      "  Tracts found: 6425\n",
      "\n",
      " Combined dataset: 25700 rows, 7 columns\n",
      "\n",
      "Years included:\n",
      "YEAR\n",
      "2018    6425\n",
      "2019    6425\n",
      "2020    6425\n",
      "2021    6425\n",
      "Name: count, dtype: int64\n",
      "Step 2: Converting to numeric\n",
      "\n",
      " All numeric columns converted\n",
      "Step 3: Calculating Tract Averages (2018-2022)\n",
      " Unique tracts: 6425\n",
      "\n",
      "======================================================================\n",
      "Step 4: Formating Tract IDs\n",
      "======================================================================\n",
      "\n",
      "Before formatting - ID examples:\n",
      "['060371011101', '060371011102', '060371011103']\n",
      "\n",
      "After formatting - ID examples:\n",
      "['060371011101', '060371011102', '060371011103']\n",
      "\n",
      " All tract IDs formatted and sorted\n",
      "Step 5: Handling Missing Values\n",
      "\n",
      "Missing values before imputation:\n",
      "  avg_traffic_pct: 0 (0.00%)\n",
      "  avg_diesel_pm: 13 (0.20%)\n",
      "  avg_cancer_risk: 13 (0.20%)\n",
      "  avg_resp_hazard: 13 (0.20%)\n",
      "  avg_ej_index: 0 (0.00%)\n",
      "\n",
      "Applying spatial nearest-neighbor imputation for avg_traffic_pct...\n",
      "\n",
      "Applying county-wide median imputation for other variables...\n",
      "  avg_diesel_pm: filled 13 values with median = 0.83\n",
      "  avg_cancer_risk: filled 13 values with median = 41.92\n",
      "  avg_resp_hazard: filled 13 values with median = 1.03\n",
      "\n",
      "Missing values after imputation:\n",
      "  avg_traffic_pct: 0\n",
      "  avg_diesel_pm: 0\n",
      "  avg_cancer_risk: 0\n",
      "  avg_resp_hazard: 0\n",
      "  avg_ej_index: 0\n",
      "\n",
      " No missing values\n",
      "(6425, 6)\n"
     ]
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "combined_df = load_and_combine_files()\n",
    "combined_df = convert_to_numeric(combined_df)\n",
    "avg_df = calculate_tract_averages(combined_df)\n",
    "avg_df = format_tract_ids(avg_df)\n",
    "avg_df = handle_missing_values(avg_df)\n",
    "\n",
    "# Save final output\n",
    "avg_df.to_csv(\"final_avg_EJ_LA_2018_2022.csv\", index=False)\n",
    "print(avg_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8c8c6-3547-42fd-91f1-96008b26ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
